<section>
  <h2>Why redesign CS1 assessment?</h2>
  <ul>
    <li class="fragment">Popular CS1 tasks (e.g., Nifty) now <em>trivially solvable</em> by code generators trained on public specs/solutions.</li>
    <li class="fragment">Risk: students bypass debugging, decomposition, and design — accumulating “cognitive debt.”</li>
    <li class="fragment">Goal: teach <strong>computational thinking</strong> + <strong>AI/tool literacy</strong> without banning AI.</li>
  </ul>
  <p class="notice">Design assessments AI can assist with — but cannot ace without <em>your</em> reasoning.</p>
  <aside class="notes">
    These projects are well-designed — they’re engaging, they teach key principles, and they’re widely used — but their very popularity has become a weakness.

    Paper highlights: LLMs produced correct Breakout using PGL despite no PGL description; suggests training exposure to assignment patterns. We respond by shifting to AI-hard subproblems centered on images, ambiguity, and interdependence.

    We also know from cognitive science research that over-reliance on AI tools leads to what’s called cognitive debt — where the tool does the thinking and the human’s own capacity for reasoning atrophies over time
  </aside>
</section>